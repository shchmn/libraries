{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a63b813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
      "     |████████████████████████████████| 26.7 MB 1.9 MB/s            \n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
      "Collecting scipy>=1.1.0\n",
      "  Downloading scipy-1.7.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.3 MB)\n",
      "     |████████████████████████████████| 39.3 MB 1.4 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.6 in ./homework/lib/python3.8/site-packages (from scikit-learn) (1.21.5)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "     |████████████████████████████████| 306 kB 2.0 MB/s            \n",
      "\u001b[?25hInstalling collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.1.0 scikit-learn-1.0.2 scipy-1.7.3 threadpoolctl-3.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29578000",
   "metadata": {},
   "source": [
    "## Задание 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "58ec4791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a87a5b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e83d44f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename', 'data_module'])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3ca8c0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = boston.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ecd9a46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b6f07016",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = boston.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3ae34d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of black people by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for line in boston.DESCR.split('\\n'):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5850f4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(data, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d392eb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(target, columns=[\"price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "046f10ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b4e687b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f7f096c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "83b2dc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "acbeb8e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ec7f7545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152, 1)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "43f0c7af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>23.6</td>\n",
       "      <td>28.648960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>32.4</td>\n",
       "      <td>36.495014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>13.6</td>\n",
       "      <td>15.411193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>22.8</td>\n",
       "      <td>25.403213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>16.1</td>\n",
       "      <td>18.855280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>20.0</td>\n",
       "      <td>23.146689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>17.8</td>\n",
       "      <td>17.392124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>14.0</td>\n",
       "      <td>14.078599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>19.6</td>\n",
       "      <td>23.036927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>16.8</td>\n",
       "      <td>20.599433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y_test     y_pred\n",
       "173    23.6  28.648960\n",
       "274    32.4  36.495014\n",
       "491    13.6  15.411193\n",
       "72     22.8  25.403213\n",
       "452    16.1  18.855280\n",
       "76     20.0  23.146689\n",
       "316    17.8  17.392124\n",
       "140    14.0  14.078599\n",
       "471    19.6  23.036927\n",
       "500    16.8  20.599433"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_test = pd.DataFrame({\"y_test\": y_test[\"price\"],\n",
    "                           \"y_pred\": y_pred.flatten()},\n",
    "                          columns=['y_test', 'y_pred'])\n",
    "check_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "321f031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5837cfb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7112260057484974"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f31b019",
   "metadata": {},
   "source": [
    "## Задание 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5d505738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ea12204e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9e614f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c4ac9604",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [{'n_estimators': [1000],\n",
    "               'max_depth': [12]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c0e8cf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(estimator=RandomForestRegressor(random_state=42),\n",
    "                   param_grid=parameters,\n",
    "                   scoring='accuracy',\n",
    "                   cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "27afde56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train.values[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e5c10f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152,)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b22ad69a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>23.6</td>\n",
       "      <td>22.743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>32.4</td>\n",
       "      <td>31.684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>13.6</td>\n",
       "      <td>16.717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>22.8</td>\n",
       "      <td>23.738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>16.1</td>\n",
       "      <td>17.376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>20.0</td>\n",
       "      <td>21.261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>17.8</td>\n",
       "      <td>19.808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>14.0</td>\n",
       "      <td>14.831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>19.6</td>\n",
       "      <td>21.306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>16.8</td>\n",
       "      <td>21.145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y_test  y_pred\n",
       "173    23.6  22.743\n",
       "274    32.4  31.684\n",
       "491    13.6  16.717\n",
       "72     22.8  23.738\n",
       "452    16.1  17.376\n",
       "76     20.0  21.261\n",
       "316    17.8  19.808\n",
       "140    14.0  14.831\n",
       "471    19.6  21.306\n",
       "500    16.8  21.145"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_test = pd.DataFrame({\"y_test\": y_test[\"price\"],\n",
    "                           \"y_pred\": y_pred.flatten()},\n",
    "                          columns=['y_test', 'y_pred'])\n",
    "check_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "89a73181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8708210657071842"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcda9d8f",
   "metadata": {},
   "source": [
    "Величина r2_score в первом задании меньше величины r2_score во втором задании, а значит, вторая модель работает лучше"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c65273",
   "metadata": {},
   "source": [
    "## Задание 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4fe1c665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    A random forest regressor.\n",
      "\n",
      "    A random forest is a meta estimator that fits a number of classifying\n",
      "    decision trees on various sub-samples of the dataset and uses averaging\n",
      "    to improve the predictive accuracy and control over-fitting.\n",
      "    The sub-sample size is controlled with the `max_samples` parameter if\n",
      "    `bootstrap=True` (default), otherwise the whole dataset is used to build\n",
      "    each tree.\n",
      "\n",
      "    Read more in the :ref:`User Guide <forest>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    n_estimators : int, default=100\n",
      "        The number of trees in the forest.\n",
      "\n",
      "        .. versionchanged:: 0.22\n",
      "           The default value of ``n_estimators`` changed from 10 to 100\n",
      "           in 0.22.\n",
      "\n",
      "    criterion : {\"squared_error\", \"absolute_error\", \"poisson\"},             default=\"squared_error\"\n",
      "        The function to measure the quality of a split. Supported criteria\n",
      "        are \"squared_error\" for the mean squared error, which is equal to\n",
      "        variance reduction as feature selection criterion, \"absolute_error\"\n",
      "        for the mean absolute error, and \"poisson\" which uses reduction in\n",
      "        Poisson deviance to find splits.\n",
      "        Training using \"absolute_error\" is significantly slower\n",
      "        than when using \"squared_error\".\n",
      "\n",
      "        .. versionadded:: 0.18\n",
      "           Mean Absolute Error (MAE) criterion.\n",
      "\n",
      "        .. versionadded:: 1.0\n",
      "           Poisson criterion.\n",
      "\n",
      "        .. deprecated:: 1.0\n",
      "            Criterion \"mse\" was deprecated in v1.0 and will be removed in\n",
      "            version 1.2. Use `criterion=\"squared_error\"` which is equivalent.\n",
      "\n",
      "        .. deprecated:: 1.0\n",
      "            Criterion \"mae\" was deprecated in v1.0 and will be removed in\n",
      "            version 1.2. Use `criterion=\"absolute_error\"` which is equivalent.\n",
      "\n",
      "    max_depth : int, default=None\n",
      "        The maximum depth of the tree. If None, then nodes are expanded until\n",
      "        all leaves are pure or until all leaves contain less than\n",
      "        min_samples_split samples.\n",
      "\n",
      "    min_samples_split : int or float, default=2\n",
      "        The minimum number of samples required to split an internal node:\n",
      "\n",
      "        - If int, then consider `min_samples_split` as the minimum number.\n",
      "        - If float, then `min_samples_split` is a fraction and\n",
      "          `ceil(min_samples_split * n_samples)` are the minimum\n",
      "          number of samples for each split.\n",
      "\n",
      "        .. versionchanged:: 0.18\n",
      "           Added float values for fractions.\n",
      "\n",
      "    min_samples_leaf : int or float, default=1\n",
      "        The minimum number of samples required to be at a leaf node.\n",
      "        A split point at any depth will only be considered if it leaves at\n",
      "        least ``min_samples_leaf`` training samples in each of the left and\n",
      "        right branches.  This may have the effect of smoothing the model,\n",
      "        especially in regression.\n",
      "\n",
      "        - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "        - If float, then `min_samples_leaf` is a fraction and\n",
      "          `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "          number of samples for each node.\n",
      "\n",
      "        .. versionchanged:: 0.18\n",
      "           Added float values for fractions.\n",
      "\n",
      "    min_weight_fraction_leaf : float, default=0.0\n",
      "        The minimum weighted fraction of the sum total of weights (of all\n",
      "        the input samples) required to be at a leaf node. Samples have\n",
      "        equal weight when sample_weight is not provided.\n",
      "\n",
      "    max_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\"\n",
      "        The number of features to consider when looking for the best split:\n",
      "\n",
      "        - If int, then consider `max_features` features at each split.\n",
      "        - If float, then `max_features` is a fraction and\n",
      "          `round(max_features * n_features)` features are considered at each\n",
      "          split.\n",
      "        - If \"auto\", then `max_features=n_features`.\n",
      "        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "        - If \"log2\", then `max_features=log2(n_features)`.\n",
      "        - If None, then `max_features=n_features`.\n",
      "\n",
      "        Note: the search for a split does not stop until at least one\n",
      "        valid partition of the node samples is found, even if it requires to\n",
      "        effectively inspect more than ``max_features`` features.\n",
      "\n",
      "    max_leaf_nodes : int, default=None\n",
      "        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "        Best nodes are defined as relative reduction in impurity.\n",
      "        If None then unlimited number of leaf nodes.\n",
      "\n",
      "    min_impurity_decrease : float, default=0.0\n",
      "        A node will be split if this split induces a decrease of the impurity\n",
      "        greater than or equal to this value.\n",
      "\n",
      "        The weighted impurity decrease equation is the following::\n",
      "\n",
      "            N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "                                - N_t_L / N_t * left_impurity)\n",
      "\n",
      "        where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "        samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "        left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "\n",
      "        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "        if ``sample_weight`` is passed.\n",
      "\n",
      "        .. versionadded:: 0.19\n",
      "\n",
      "    bootstrap : bool, default=True\n",
      "        Whether bootstrap samples are used when building trees. If False, the\n",
      "        whole dataset is used to build each tree.\n",
      "\n",
      "    oob_score : bool, default=False\n",
      "        Whether to use out-of-bag samples to estimate the generalization score.\n",
      "        Only available if bootstrap=True.\n",
      "\n",
      "    n_jobs : int, default=None\n",
      "        The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
      "        :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
      "        trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      "        context. ``-1`` means using all processors. See :term:`Glossary\n",
      "        <n_jobs>` for more details.\n",
      "\n",
      "    random_state : int, RandomState instance or None, default=None\n",
      "        Controls both the randomness of the bootstrapping of the samples used\n",
      "        when building trees (if ``bootstrap=True``) and the sampling of the\n",
      "        features to consider when looking for the best split at each node\n",
      "        (if ``max_features < n_features``).\n",
      "        See :term:`Glossary <random_state>` for details.\n",
      "\n",
      "    verbose : int, default=0\n",
      "        Controls the verbosity when fitting and predicting.\n",
      "\n",
      "    warm_start : bool, default=False\n",
      "        When set to ``True``, reuse the solution of the previous call to fit\n",
      "        and add more estimators to the ensemble, otherwise, just fit a whole\n",
      "        new forest. See :term:`the Glossary <warm_start>`.\n",
      "\n",
      "    ccp_alpha : non-negative float, default=0.0\n",
      "        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      "        subtree with the largest cost complexity that is smaller than\n",
      "        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      "        :ref:`minimal_cost_complexity_pruning` for details.\n",
      "\n",
      "        .. versionadded:: 0.22\n",
      "\n",
      "    max_samples : int or float, default=None\n",
      "        If bootstrap is True, the number of samples to draw from X\n",
      "        to train each base estimator.\n",
      "\n",
      "        - If None (default), then draw `X.shape[0]` samples.\n",
      "        - If int, then draw `max_samples` samples.\n",
      "        - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
      "          `max_samples` should be in the interval `(0.0, 1.0]`.\n",
      "\n",
      "        .. versionadded:: 0.22\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    base_estimator_ : DecisionTreeRegressor\n",
      "        The child estimator template used to create the collection of fitted\n",
      "        sub-estimators.\n",
      "\n",
      "    estimators_ : list of DecisionTreeRegressor\n",
      "        The collection of fitted sub-estimators.\n",
      "\n",
      "    feature_importances_ : ndarray of shape (n_features,)\n",
      "        The impurity-based feature importances.\n",
      "        The higher, the more important the feature.\n",
      "        The importance of a feature is computed as the (normalized)\n",
      "        total reduction of the criterion brought by that feature.  It is also\n",
      "        known as the Gini importance.\n",
      "\n",
      "        Warning: impurity-based feature importances can be misleading for\n",
      "        high cardinality features (many unique values). See\n",
      "        :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "\n",
      "    n_features_ : int\n",
      "        The number of features when ``fit`` is performed.\n",
      "\n",
      "        .. deprecated:: 1.0\n",
      "            Attribute `n_features_` was deprecated in version 1.0 and will be\n",
      "            removed in 1.2. Use `n_features_in_` instead.\n",
      "\n",
      "    n_features_in_ : int\n",
      "        Number of features seen during :term:`fit`.\n",
      "\n",
      "        .. versionadded:: 0.24\n",
      "\n",
      "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "        Names of features seen during :term:`fit`. Defined only when `X`\n",
      "        has feature names that are all strings.\n",
      "\n",
      "        .. versionadded:: 1.0\n",
      "\n",
      "    n_outputs_ : int\n",
      "        The number of outputs when ``fit`` is performed.\n",
      "\n",
      "    oob_score_ : float\n",
      "        Score of the training dataset obtained using an out-of-bag estimate.\n",
      "        This attribute exists only when ``oob_score`` is True.\n",
      "\n",
      "    oob_prediction_ : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      "        Prediction computed with out-of-bag estimate on the training set.\n",
      "        This attribute exists only when ``oob_score`` is True.\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    sklearn.tree.DecisionTreeRegressor : A decision tree regressor.\n",
      "    sklearn.ensemble.ExtraTreesRegressor : Ensemble of extremely randomized\n",
      "        tree regressors.\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    The default values for the parameters controlling the size of the trees\n",
      "    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      "    unpruned trees which can potentially be very large on some data sets. To\n",
      "    reduce memory consumption, the complexity and size of the trees should be\n",
      "    controlled by setting those parameter values.\n",
      "\n",
      "    The features are always randomly permuted at each split. Therefore,\n",
      "    the best found split may vary, even with the same training data,\n",
      "    ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      "    of the criterion is identical for several splits enumerated during the\n",
      "    search of the best split. To obtain a deterministic behaviour during\n",
      "    fitting, ``random_state`` has to be fixed.\n",
      "\n",
      "    The default value ``max_features=\"auto\"`` uses ``n_features``\n",
      "    rather than ``n_features / 3``. The latter was originally suggested in\n",
      "    [1], whereas the former was more recently justified empirically in [2].\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "    .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      "\n",
      "    .. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n",
      "           trees\", Machine Learning, 63(1), 3-42, 2006.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.ensemble import RandomForestRegressor\n",
      "    >>> from sklearn.datasets import make_regression\n",
      "    >>> X, y = make_regression(n_features=4, n_informative=2,\n",
      "    ...                        random_state=0, shuffle=False)\n",
      "    >>> regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
      "    >>> regr.fit(X, y)\n",
      "    RandomForestRegressor(...)\n",
      "    >>> print(regr.predict([[0, 0, 0, 0]]))\n",
      "    [-8.32987858]\n",
      "    \n"
     ]
    }
   ],
   "source": [
    " print(RandomForestRegressor.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4051c69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a8b3561b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dc0d43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
